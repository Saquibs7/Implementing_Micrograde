# Implementing_Micrograde
the code consist of two parts 
part 1: Implementing an autograde system - Micrograde Goal : to create an automatic differentiation system that tracks mathematical operations, builds a computational graph, and calculates gradients
part 2: part 2: manually implement backpropagation for a 2-layer Multi-Layer Perceptron (MLP) with batch normalization, cross-entropy loss, and tanh activation to gain insights into how neural networks learn by manually following the steps of backpropagation
