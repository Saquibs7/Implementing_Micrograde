{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b75030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a6c09",
   "metadata": {},
   "source": [
    "### part 1: Implementing an autograde system - Micrograde \n",
    "### Goal : to create an automatic differentiation system that tracks mathematical operations, builds a computational graph,   and calculates gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ddae32",
   "metadata": {},
   "source": [
    "#### Variable class will represent each variable in the computational graph. It will store the value, gradient, and any operation that created it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19b18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, grad=0.0, _prev=(), _op=''):\n",
    "        self.value = value  # The actual value of the variable\n",
    "        self.grad = grad    # Gradient of the variable (initialized to 0)\n",
    "        self._prev = set(_prev)  # Previous variables (inputs) used to create this variable\n",
    "        self._op = _op      # The operation that produced this variable (for visualization)\n",
    "        self._backward = lambda: None  # Function to compute the gradient for this variable\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Create a new Variable for the sum\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.value + other.value, _prev=(self, other), _op='+')\n",
    "        \n",
    "        # Define the backward pass for addition\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Create a new Variable for the product\n",
    "        other = other if isinstance(other, Variable) else Variable(other)\n",
    "        out = Variable(self.value * other.value, _prev=(self, other), _op='*')\n",
    "        \n",
    "        # Define the backward pass for multiplication\n",
    "        # The backward function performs backpropagation by computing gradients for each variable in the computational graph\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # The backward pass defines how gradients are calculated for each operation using the chain rule.\n",
    "    def backward(self):\n",
    "        # Set the gradient of the output variable to 1\n",
    "        self.grad = 1.0\n",
    "        # Perform a topological sort to ensure correct gradient computation order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "                \n",
    "        build_topo(self)\n",
    "\n",
    "        # Traverse backward in reverse topological order\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Variable(value={self.value}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0710d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of y: Variable(value=21.0, grad=1.0)\n",
      "Gradient of x: Variable(value=5.0, grad=3.0)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(5.0)\n",
    "y = (x + 2) * 3\n",
    "# Perform backpropagation\n",
    "y.backward()\n",
    "print(\"Value of y:\", y)\n",
    "print(\"Gradient of x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f98d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\saquib hussain\\anaconda3\\lib\\site-packages (0.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "389e58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5abd8c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computational_graph.png'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def draw_graph(var):\n",
    "    dot = Digraph(format='png', graph_attr={'rankdir': 'LR'})\n",
    "    \n",
    "    # Recursive function to add nodes and edges to the graph\n",
    "    def add_nodes_edges(v):\n",
    "        if v not in seen:\n",
    "            # Add the node for the variable\n",
    "            seen.add(v)\n",
    "            node_id = str(id(v))\n",
    "            dot.node(node_id, f\"{v._op} | {v.value:.4f} | grad={v.grad:.4f}\", shape='record')\n",
    "            \n",
    "            # Add edges for each previous variable\n",
    "            for child in v._prev:\n",
    "                child_id = str(id(child))\n",
    "                dot.edge(child_id, node_id)\n",
    "                add_nodes_edges(child)\n",
    "    \n",
    "    seen = set()\n",
    "    add_nodes_edges(var)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Draw the graph for y\n",
    "dot = draw_graph(y)\n",
    "dot.render('computational_graph', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9da4d9",
   "metadata": {},
   "source": [
    "## part 2: manually implement backpropagation for a 2-layer Multi-Layer Perceptron (MLP) with batch normalization, cross-entropy loss, and tanh activation to gain insights into how neural networks learn by manually following the steps of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2876097",
   "metadata": {},
   "source": [
    "## Setting the MLP structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5ca7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input  # Store input for use in backward pass\n",
    "        self.output = input.dot(self.weights) + self.biases  # Store output for accuracy calculation\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Compute gradients for weights, biases, and input\n",
    "        self.grad_weights = self.input.T.dot(grad_output)\n",
    "        self.grad_biases = np.sum(grad_output, axis=0)\n",
    "        grad_input = grad_output.dot(self.weights.T)\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87ca35",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122fe4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the inputs to have zero mean and unit variance, making training more stable and efficient\n",
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5):\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Store input for use in backward pass\n",
    "        self.input = input\n",
    "        \n",
    "        # Calculate mean and variance for normalization\n",
    "        self.mean = np.mean(input, axis=0)\n",
    "        self.var = np.var(input, axis=0)\n",
    "        \n",
    "        # Normalize input\n",
    "        self.x_normalized = (input - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        output = self.gamma * self.x_normalized + self.beta\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Get batch size\n",
    "        N = self.input.shape[0]\n",
    "        \n",
    "        # Compute gradients with respect to gamma and beta\n",
    "        self.grad_gamma = np.sum(grad_output * self.x_normalized, axis=0)\n",
    "        self.grad_beta = np.sum(grad_output, axis=0)\n",
    "\n",
    "        # Backpropagate through normalization\n",
    "        dx_normalized = grad_output * self.gamma\n",
    "        dvar = np.sum(dx_normalized * (self.input - self.mean) * -0.5 * np.power(self.var + self.epsilon, -1.5), axis=0)\n",
    "        dmean = np.sum(dx_normalized * -1.0 / np.sqrt(self.var + self.epsilon), axis=0) + dvar * np.mean(-2.0 * (self.input - self.mean), axis=0)\n",
    "\n",
    "        dx = dx_normalized / np.sqrt(self.var + self.epsilon) + dvar * 2.0 * (self.input - self.mean) / N + dmean / N\n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549d987",
   "metadata": {},
   "source": [
    "### Implementing the activation fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc88b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhActivation:\n",
    "    def forward(self, x):\n",
    "        self.output = np.tanh(x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Derivative of tanh is 1 - tanh^2(x)\n",
    "        return grad_output * (1 - self.output ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f3207",
   "metadata": {},
   "source": [
    "### Calculating cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2743719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, predictions, targets):\n",
    "        # Apply softmax to predictions\n",
    "        self.predictions = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
    "        self.predictions /= np.sum(self.predictions, axis=1, keepdims=True)\n",
    "        # Compute the loss\n",
    "        N = targets.shape[0]\n",
    "        self.targets = targets\n",
    "        loss = -np.sum(targets * np.log(self.predictions + 1e-15)) / N\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        # Gradient of cross-entropy loss with softmax\n",
    "        N = self.targets.shape[0]\n",
    "        grad_output = (self.predictions - self.targets) / N\n",
    "        return grad_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e3424",
   "metadata": {},
   "source": [
    "### Assembling everything to make two layer MLP to implement forward and backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf0ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Define the layers\n",
    "        self.layer1 = LinearLayer(input_size, hidden_size)\n",
    "        self.batchnorm1 = BatchNormalization(hidden_size)\n",
    "        self.activation1 = TanhActivation()\n",
    "        self.layer2 = LinearLayer(hidden_size, output_size)\n",
    "        self.batchnorm2 = BatchNormalization(output_size)\n",
    "        self.loss = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x, targets):\n",
    "        # Forward pass\n",
    "        out = self.layer1.forward(x)\n",
    "        out = self.batchnorm1.forward(out)\n",
    "        out = self.activation1.forward(out)\n",
    "        out = self.layer2.forward(out)\n",
    "        out = self.batchnorm2.forward(out)\n",
    "        loss = self.loss.forward(out, targets)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        # Backward pass\n",
    "        grad_output = self.loss.backward()\n",
    "        grad_output = self.batchnorm2.backward(grad_output)\n",
    "        grad_output = self.layer2.backward(grad_output)\n",
    "        grad_output = self.activation1.backward(grad_output)\n",
    "        grad_output = self.batchnorm1.backward(grad_output)\n",
    "        grad_output = self.layer1.backward(grad_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043c6f2",
   "metadata": {},
   "source": [
    "### Testing the MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0def8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9381647656067867\n",
      "Gradient of first layer weights:\n",
      " [[  3.04320263  -2.24298385  -1.39316963  -5.66049313   8.20292955]\n",
      " [  3.49245225  -2.98273095   0.46860402  -2.5291769   -1.29853867]\n",
      " [-10.3840303   -9.84227041  -1.41877909  -4.45135564 -11.42525166]]\n",
      "Accuracy: 60.0 %\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(10, 3)  # 10 samples, 3 features\n",
    "y = np.zeros((10, 2))       # Binary classification (2 output classes)\n",
    "y[np.arange(10), np.random.randint(0, 2, size=10)] = 1  # One-hot encoding for targets\n",
    "\n",
    "# Define the accuracy function\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    pred_labels = np.argmax(predictions, axis=1)  # Class with highest probability\n",
    "    true_labels = np.argmax(targets, axis=1)      # True labels\n",
    "    accuracy = np.mean(pred_labels == true_labels) * 100  # Percentage accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Create the MLP\n",
    "mlp = MLP(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# Forward pass\n",
    "loss = mlp.forward(X, y)\n",
    "\n",
    "# Calculate predictions and accuracy\n",
    "predictions = mlp.layer2.output  # Output layer activations\n",
    "accuracy = calculate_accuracy(predictions, y)\n",
    "\n",
    "# Backward pass\n",
    "mlp.backward()\n",
    "\n",
    "# Output the loss, gradients, and accuracy\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Gradient of first layer weights:\\n\", mlp.layer1.grad_weights)\n",
    "print(\"Accuracy:\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea4f6b",
   "metadata": {},
   "source": [
    "### we can clearly see that accuracy is 60 % becuase of following reasons\n",
    "#### 1. we are using very less data set that is of 10 rows\n",
    "#### 2. we go through only 1 epoch\n",
    "#### 3. not using optimization algorithm like gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbe1c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.28518137879923966, Accuracy: 80.0%\n",
      "Epoch 2/10, Loss: 0.31056300092884503, Accuracy: 80.0%\n",
      "Epoch 3/10, Loss: 0.28213459184854156, Accuracy: 80.0%\n",
      "Epoch 4/10, Loss: 0.3066689973028275, Accuracy: 80.0%\n",
      "Epoch 5/10, Loss: 0.2792739391772824, Accuracy: 80.0%\n",
      "Epoch 6/10, Loss: 0.30292140876107176, Accuracy: 80.0%\n",
      "Epoch 7/10, Loss: 0.27659827725459285, Accuracy: 80.0%\n",
      "Epoch 8/10, Loss: 0.2993409089012876, Accuracy: 90.0%\n",
      "Epoch 9/10, Loss: 0.2740621480535676, Accuracy: 80.0%\n",
      "Epoch 10/10, Loss: 0.2958966945590865, Accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs to train the model\n",
    "num_epochs = 10\n",
    "\n",
    "learning_rate = 0.01  # Define a learning rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    loss = mlp.forward(X, y)\n",
    "    \n",
    "    # Calculate predictions and accuracy for monitoring\n",
    "    predictions = mlp.layer2.output\n",
    "    accuracy = calculate_accuracy(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    mlp.backward()\n",
    "    \n",
    "    # Gradient Descent Step (parameter update)\n",
    "    # For each layer, we update weights and biases with the gradients\n",
    "    mlp.layer1.weights -= learning_rate * mlp.layer1.grad_weights\n",
    "    mlp.layer1.biases -= learning_rate * mlp.layer1.grad_biases\n",
    "    mlp.layer2.weights -= learning_rate * mlp.layer2.grad_weights\n",
    "    mlp.layer2.biases -= learning_rate * mlp.layer2.grad_biases\n",
    "    \n",
    "    # Print loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25621108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
